{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:04<00:00, 2.04MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 56.8kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:07<00:00, 232kB/s] \n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.25MB/s]\n"
     ]
    }
   ],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_data = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1=nn.Linear(784,256)\n",
    "        self.l2=nn.Linear(256, 10)\n",
    "        self.relu=nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=x.view(-1,784)\n",
    "        x=self.relu(self.l1(x))\n",
    "        x=self.l2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simpleNN().to(device)\n",
    "\n",
    "def noofparams(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "totalbefore,trainablebefore=noofparams(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, epochs=1, max_iters=None):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total = 0\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(loader, total=max_iters or len(loader))\n",
    "        for x, y in loop:\n",
    "            if max_iters and total >= max_iters:\n",
    "                return\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            total += 1\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct = [0]*10\n",
    "    total = [0]*10\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            preds = out.argmax(dim=1)\n",
    "            for i in range(len(y)):\n",
    "                total[y[i]] += 1\n",
    "                if preds[i] == y[i]:\n",
    "                    correct[y[i]] += 1\n",
    "    for i in range(10):\n",
    "        print(f\"Digit {i} Accuracy: {correct[i]}/{total[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before LORA\n",
      "Digit 0 Accuracy: 10/980\n",
      "Digit 1 Accuracy: 623/1135\n",
      "Digit 2 Accuracy: 42/1032\n",
      "Digit 3 Accuracy: 58/1010\n",
      "Digit 4 Accuracy: 1/982\n",
      "Digit 5 Accuracy: 117/892\n",
      "Digit 6 Accuracy: 33/958\n",
      "Digit 7 Accuracy: 40/1028\n",
      "Digit 8 Accuracy: 2/974\n",
      "Digit 9 Accuracy: 1/1009\n",
      "no. of total params: 203530\n",
      "no. of trainable params: 203530\n"
     ]
    }
   ],
   "source": [
    "print('before LORA')\n",
    "evaluate(model)\n",
    "print(\"no. of total params:\",totalbefore)\n",
    "print(\"no. of trainable params:\",trainablebefore)\n",
    "\n",
    "\n",
    "original_weights = {name: param.detach().clone() for name, param in model.named_parameters()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lora(nn.Module):\n",
    "    def __init__(self,indim,outdim,rank=1,alpha=1):\n",
    "        super().__init__()\n",
    "        self.lora_a=nn.Parameter(torch.randn(rank,outdim))\n",
    "        self.lora_b=nn.Parameter(torch.zeros(indim,rank))\n",
    "        self.scale=alpha/rank\n",
    "        self.enabled=True\n",
    "    \n",
    "    def forward(self,w):\n",
    "        if self.enabled:\n",
    "            return w+(self.lora_b @ self.lora_a).view(w.shape) * self.scale\n",
    "        else :\n",
    "            return w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora(layer):\n",
    "    in_f, out_f = layer.weight.shape\n",
    "    parametrize.register_parametrization(layer, \"weight\", Lora(in_f, out_f, rank=1, alpha=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_lora(model.l1)\n",
    "apply_lora(model.l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toggle_lora(enabled):\n",
    "    for layer in [model.l1, model.l2]:\n",
    "        layer.parametrizations[\"weight\"][0].enabled = enabled\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' not in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after LORA\n",
      "Digit 0 Accuracy: 10/980\n",
      "Digit 1 Accuracy: 623/1135\n",
      "Digit 2 Accuracy: 42/1032\n",
      "Digit 3 Accuracy: 58/1010\n",
      "Digit 4 Accuracy: 1/982\n",
      "Digit 5 Accuracy: 117/892\n",
      "Digit 6 Accuracy: 33/958\n",
      "Digit 7 Accuracy: 40/1028\n",
      "Digit 8 Accuracy: 2/974\n",
      "Digit 9 Accuracy: 1/1009\n",
      "no. of total params: 204836\n",
      "no. of trainable params: 1306\n"
     ]
    }
   ],
   "source": [
    "def noofparams(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "totalafter,trainableafter=noofparams(model)\n",
    "print('after LORA')\n",
    "evaluate(model)\n",
    "print(\"no. of total params:\",totalafter)\n",
    "print(\"no. of trainable params:\",trainableafter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## also sanity check\n",
    "assert torch.allclose(model.l1.parametrizations.weight.original, original_weights['l1.weight'])\n",
    "assert torch.allclose(model.l2.parametrizations.weight.original, original_weights['l2.weight'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
